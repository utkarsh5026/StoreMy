# Benchmarking Guidelines for StoreMy Project

## Overview
When asked to create benchmarks for any component in the StoreMy project, follow these standardized practices to ensure consistency, proper organization, and professional presentation.

## File Organization

All benchmark results must be saved to the `.benchmarks/` directory with the following structure:

```
.benchmarks/
├── json/
│   ├── <component_name>_<timestamp>.json
│   └── <component_name>_latest.json
└── html/
    ├── <component_name>_<timestamp>.html
    └── <component_name>_latest.html
```

**Example:**
- `.benchmarks/json/aggregation_operator_2025-10-04_07-26-15.json`
- `.benchmarks/json/aggregation_operator_latest.json`
- `.benchmarks/html/aggregation_operator_2025-10-04_07-26-15.html`
- `.benchmarks/html/aggregation_operator_latest.html`

## Benchmark Test Implementation

### 1. Test Function Naming
```go
func TestBenchmarkAndSaveResults(t *testing.T) {
    // Benchmark implementation
}
```

### 2. Result Structure
```go
type BenchmarkResult struct {
    Name              string        `json:"name"`
    Operations        int           `json:"operations"`
    TotalTime         time.Duration `json:"total_time_ns"`
    NsPerOp           int64         `json:"ns_per_op"`
    BytesPerOp        int64         `json:"bytes_per_op"`
    AllocsPerOp       int64         `json:"allocs_per_op"`
    MemAllocBytes     int64         `json:"mem_alloc_bytes"`
    MemTotalAllocated int64         `json:"mem_total_allocated"`
}
```

### 3. Directory Creation
Always create both `json/` and `html/` subdirectories:

```go
benchmarkDir := filepath.Join("..", "..", "..", ".benchmarks")
jsonDir := filepath.Join(benchmarkDir, "json")
htmlDir := filepath.Join(benchmarkDir, "html")

os.MkdirAll(jsonDir, 0755)
os.MkdirAll(htmlDir, 0755)
```

### 4. Timestamp Format
Use consistent timestamp format: `2006-01-02_15-04-05`

```go
timestamp := time.Now().Format("2006-01-02_15-04-05")
```

## HTML Report Requirements

### Styling
- **Framework:** Tailwind CSS (via CDN)
- **Font:** Cascadia Mono (with fallbacks: Cascadia Code, Consolas, Monaco, Courier New, monospace)
- **Theme:** Dark mode (bg-gray-900 background)

### Required Sections

1. **Header**
   - Component name as title
   - Timestamp of generation
   - Dark background with blue accent colors

2. **Summary Cards**
   - Fastest operation
   - Total number of tests
   - Average memory per operation
   - Use colored highlights (green for good, red for concerning)

3. **Results Table**
   - Columns: Benchmark name, Operations, Time/Op, Bytes/Op, Allocs/Op, Total Time
   - Color-coded performance (green for fast < 60μs, red for slow > 80μs)
   - Hover effects for better UX
   - Alternating row colors

4. **Performance Insights**
   - Fastest and slowest operations
   - Performance comparisons
   - Scaling characteristics
   - Data-driven insights with colored highlights

5. **Footer**
   - "Generated by StoreMy Benchmark Suite"

### HTML Template Structure
```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>[Component] Benchmark Results</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        body {
            font-family: 'Cascadia Mono', 'Cascadia Code', Consolas, Monaco, 'Courier New', monospace;
        }
    </style>
</head>
<body class="bg-gray-900 text-gray-100 p-8">
    <!-- Content here -->
</body>
</html>
```

## Helper Functions Required

All benchmark tests should include these helper functions:

1. **`generateBenchmarkHTML(results []BenchmarkResult, timestamp string) string`**
   - Generates complete HTML report

2. **`getFastestOp(results []BenchmarkResult) string`**
   - Identifies fastest operation

3. **`getAvgMemory(results []BenchmarkResult) int64`**
   - Calculates average memory usage

4. **`formatNumber(n int64) string`**
   - Formats numbers with comma separators (e.g., 1,000,000)

5. **`formatDuration(d time.Duration) string`**
   - Formats duration with appropriate units (ns, μs, ms, s)

6. **`generateInsights(results []BenchmarkResult) string`**
   - Creates performance insights with comparisons

7. **`getScalingType(r1, r2 BenchmarkResult) string`**
   - Analyzes scaling characteristics (sub-linear, linear, super-linear)

## Output Requirements

### Console Output
```
[Benchmark_Name]: [ops] ops, [total_time] total, [ns_per_op] ns/op, [bytes_per_op] B/op, [allocs_per_op] allocs/op

Benchmark results saved to:
  JSON:
    - [path_to_timestamped_json]
    - [path_to_latest_json]
  HTML:
    - [path_to_timestamped_html]
    - [path_to_latest_html]
```

### File Outputs
1. **Timestamped files:** For historical tracking and comparison
2. **Latest files:** For quick access to most recent results

## Memory Measurement

Use `runtime.MemStats` for accurate memory tracking:

```go
var memStatsBefore, memStatsAfter runtime.MemStats
runtime.ReadMemStats(&memStatsBefore)

// Run benchmark operations

runtime.ReadMemStats(&memStatsAfter)
```

## Best Practices

1. **Test Different Scales:** Include benchmarks with varying data sizes (e.g., 100, 1000, 10000 items)
2. **Multiple Operations:** Test different operation types to compare performance
3. **Sufficient Iterations:** Run enough iterations for statistical significance
4. **Reset Timer Properly:** Use `b.StopTimer()` and `b.StartTimer()` to exclude setup time
5. **Clean Resources:** Always close/cleanup resources between iterations

## Running Benchmarks

Standard command:
```bash
go test -v -run TestBenchmarkAndSaveResults ./pkg/[component_path]
```

For Go's built-in benchmark tool:
```bash
go test -bench=. -benchmem ./pkg/[component_path]
```

## Permissions

Update `.claude/settings.local.json` to allow opening HTML reports:
```json
{
  "permissions": {
    "allow": [
      "Bash(start .benchmarks/html/*.html)"
    ]
  }
}
```

## Example Reference

See `pkg/execution/aggregation/operator_bench_test.go` for a complete implementation example.

---

**Note:** These guidelines ensure all benchmarks across the project maintain consistency in presentation, storage, and analysis. When creating new benchmarks, copy the helper functions and HTML generation logic from existing benchmarks and adapt the component-specific parts.
